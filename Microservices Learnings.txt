Microservice Project

Product Service: Create and View Products, acts as Product Catalog.

Order Service: Can Order Products.

Inventory Service: Can check if product is in stock or not.

Notification Service: Can send notifications, after order is placed.

** Order Service, Inventory Service, Notification Service are going to interact with each other.

** Will have the Synchronous and Asynchronous Communication.


For Asynchronous Communication we are going to use of 2 softwares Message Queues like Kafka and Rabbit MQ's

We will use the KeyCloak as Authorization server

we will use Vault to store the secrets.

Zipkin for distributed tracing

For logging we will use Elasticsearch, logstash, Kibana.

Create the Product Service From Spring Initializer and add dependencies like spring cloud, spring web, eureka client, Lombok, mongodb

setup the mongo db in Product service and configure it in application.properties (uri, username, password, 
uri = mongodb://localhost:27817/product-service



Create a model pkg and create a Product Class there. make it Document
@Document(value = "Product")
Add All Lombok annotations

** Learn about @Builder

Product has @ID id, name, description, price

Create a repository package and create an interface as ProductRepository extends MongoRepository<Product,String>

Create a Controller package and create ProductController, annotate it with RestController, RequestMapping("/api/product")

create Api's for create, get, getAll (use @ResponseStatus for returning response)

create a ProductRequest DTO (have all fields except id because we will autogenerate it)

create service package and ProductService class to create methods related to them.

use builder method to build the Product Object

use slf4j for logging

create ProductResponse DTO for response, return everything, even id because we can use it in Order or Product identification,


Write the tests for the Product service in test package-> ProductServiceApplicationTests class
we are going the write the Integration test.

first install a library called TestContainers -> Java Library that supports the unit tests.


Learn about selenium.

add the maven dependency of TestContainers

BOM -> Bill of Materials
if we want to put the same version for all the components like for Kafka, MongoDB then use BOM dependency
take the mongodb maven dependency from TestContainers documentation
and remove the version because we have defined it in BOM dependency, otherwise it may cause issue
as we are using the JUnit 5 for testing so we need that dependency too for TestContainers and put the junit-jupiter dependency and remove the version

Let's start writing the integration test
add @TestContainers Annotation on top of the class and define the MongoDb Container inside the Test class and create its object 

MongoDbContainer is deprecated because we cannot use the NoArgsConstructor anymore, we have to manually specify the version of the MongoDbContainer, so for that we need to provide the docker image name, we are providing as "mongo:4.4.2" so this is the mongo version we want to use for test. add @Container at top of the MongoDbContainer instance and make it static .

create the static setProperties method and add annotation @DynamicPropertySource with DynamicPropertyRegistry parameter and call the add method with 2 parameters, one is uri "spring.data.mongodb.uri", mongodbcontainer::getReplicaSetUrl)
 


so at the time of starting the integration test, first test will start the mongodbcontainer with specified docker image and then it will get the replicaSetUrl and add to mongodb.uri property at time of creating the test,
we are taking the image for mongodb for testing because we dont want to use our local db




Now we are creating the test case for createProduct api in which we are expecting the ResponseStatus.Created as Response
@Test
void shouldCreateProduct() throws JsonProcessingException{

ProductRequest productRequest = getProductRequest() //but content method needs a string not pojo, so we can use the ObjectMapper object which will convert the POJO in to JSON, Autowire the Object Mapper

String productJson = objectMapper.writeValueAsString(productRequest);

mockMVC.perform(MockMvcRequestBuilders.post("/api/product")) 
	.contentType(MediaType.Application_JSON)
	.content(productJson)
	.andExpect(status().isCreated());

Assertions.assertEquals(1,productRepository.findAll().size());
}

okay now run the test

ProductRequest getProductRequest(){
return ProductRequest.builder().name("iPhone 13").description("iPhone 13").price(1200).build();
}


to be able to make a request from our integration test we need to use the MOCK MVC which will provide us the mocked servlet environment where we can call our controllers
Autowired the MockMVC mockMVC object, it will give error because we need to auto configure the MOCKMVC, add @AutoConfigureMockMVC at top of the class, now use object in methods

There is a playlist on JUnit5 and Mockito testing, go check it out 


EXERCISE:

Create the getProduct Test 



Now Create the ORDER Service

add dependencies -> Spring Web, Lombok, MySQL Driver, Data JPA 

now create the model package and ORDER Entity with fields -> id, orderNumber, List of OrderLineItems which as skucode, price, quantity and make relationship

create the Order Controller add @RestController, @RequestMapping("/api/order")

create a PostMethod as placeOrder(OrderRequest) which will return the String as "order placed succesffully", OrderRequest has the List of OrderLineItemsDTO 


create the test cases.

change the port because previous microservice is on 8080 by default


see now the order is placed but now we need to check the inventory is available or not, so need to create the Inventory Service

create a new service as InventoryService with dependencies -> Spring web, lombok, spring data jpa, mysql


QUESTION:
he didn't add the inventory service in parent folder where other services are, he said he is facing some issues, we will add them to one project using maven multimodules concept??????/

create the Entity Inventory with table name 't_inventory' with fields like id,skucode, quantity
create repo, controller, service

create a getmapping method as isInStock which returns true or false -> takes the skucode as path variable

when to get the value using path variable???? I can take using requestParam too right??? 

create a service and provide the implementation of isInStock method.
create method findBySkucode which will return the Optional of Inventory then use the isPresent method in service
added the @Transactional(readOnly = true)
create 2 Inventory object in mainapplication.class with CommandLineRunner Bean (Explore it). loadData method with InventoryRepository parameter.




================== CommandLineRunner Bean ================
special type of bean used to execute code after the SpringApplicationContext is fully initialized.
Typically used to run some initial logic( e.g loading data, database initialization, printing debug info).
Test or trigger some Logic at application startup.

we can define multiple CommandLineRunner Bean and also arrange them in order with @Order(1),@Order(2)


================= Clubbing the Projects =============
Create a new Maven Project -> go to file -> new project-> Maven -> next -> name ="Microservices New"
Now we are going to create 3 modules -> one for product, one for Order, and one for Inventory
Right click on project -> new -> module -> name as product-service also create for order and inventory service, delete the src folder
now we need to migrate every services under this microservices-new project
open the Product service copy all the dependencies and paste it in product-service which is in microservice-new, do same for order and inventory 
and add dependencymanagement tag in root pom of the project which is pom of microservices-new so that it could use for every services
In previous project out parent is org.springframework.boot, so in microservices-new project which we create as maven project doesn't have this parent, copy the parent from previous one and paste it in parent pom of  microservice-new.
copy the build tag and paste it in parent pom


now need to copy the sourcecode, for that delete the src folder for all services, take the src folders from previous projects and paste it
just to verify, everything is working fine, we can run a command as mvn clean verify, click the maven button on right of the intellij, click on 'm' icon and type  mvn clean verify



================== Need to learn the Liquibase ===================

================ Communication Between Services ==========================
2 types of communication
	Synchronous	Asynchronous

Order service will have the Synchronous communication with Inventory, and Async with Notification service

 ==== Inter Process Communication ===
Order Service will hit a http request to Inventory service and will get response in status of the product that the product is in stock or not, so based on this response the Order Service will either place the order successfully or will through exception or some error that the product is out of stock
so this kind of communication where the service A is waiting for the response of Service B is known as Synchronous Communication.

this kind of request(synchronous communication) are done by http clients (RestTemplate, WebClient). RestTemplate is by default in spring boot framework and we have another client as WebClient it comes from spring WebFlux project
RestTemplate is in maintanance mode, so Spring boot recommends to use the WebClient, and it also supports sync, async and streaming scenarios. 

Asynchronous is the "Fire and forget" approach 

Let's go to code where the Order service will call the Inventory service

we have created placeOrder method in Order Service so before saving the Order we need to first communicate with Inventory service and ask that is Inventory Available by calling its isAvailable method which will return boolean? then we will save or provide Exception.
configure the WebClient in order service, create a package name as config and create a java class as WebClientConfig, add the @Configuration  and define the Bean of WebClient, we cannot find the WebCient class because its part of WebFlux so we need to add its dependency, @Bean annotation create the bean with name which is the method name
now use the WebClient in order Service class use it in placeOrder method
By defaulte the client will hit the Asynchronous request, for synchronous request we have to call the block() method
Boolean isInStock = webClient.get().uri("http://localhost:8082/api/inventory/{skucode}").retrive().bodyToMono(Boolean.class).block();  
if(isInStock){
save
}else{
exception
}

but there is a problem, Inventory url is taking the skucode in path url and our order has multiple skucode, so we will not hit the http request for each skucode which is time taking and not a good practice 
we can collect all the skucode in one list and then we can provide it to inventory service api, need to modify the isInStock method of Inventory service where we take the list in path variable
/api/inventory/iphone13 -> this will be the url if there is one skucode, if more then /inventory/iphone13,iphone13-red -> this is how the path variable format looks like

and if we use the requestParam to get the skucode then it will look like: /api/inventory?sku-code = iphone13 & sku-code = iphone13-red 
we should go for request param format because it is more readable 

remove the path variable and add @RequestParam List<String> skucode and update other logic
create the Inventory Response DTO which will have the skucode and isInStock and method will return the List of InventoryResponse
now in Order service first collect all the squcode from the order and pass it

.uri("http://localhost:8082/api/inventory/{skucode}", uriBuilder -> uriBuilder.queryParam("skuCode",skuCodes).build().
and to store the response we also need to create the InventoryResponse class and get the array of them and implement the logic 




================ Spring Cloud =========
now before we were using the localhost:8082 or others to call the api, so the problem is, it works when its in our local machine, we need to deploy them on cloud and in cloud environment we cannot have a dedicated IP addresses, everything should be dynamic

we can have multiple instances of Inventory service and each instances can have a dynamic IP addresses, so in this case how our order service will know which instance to call, even if we can hardcored any instance because that instance can go down so for that we have a ServiceDiscovery pattern.
ServiceDiscovery is a concept of creating a server called as DiscoveryServer where it has all information of services that are registered like name,IP Address, Status(up,down)  

====communication with Service Discovery ========

Order service will first make a call to Discovery Server asking where I can find the Inventory service and discovery server will respond with particular IP address and then order service will make the call to the Inventory service, this is how we can avoid the hardcoded service 
when making the initial call to discovery server, discovery server will also send the registry as the response to the client , client will then store the local copy of discover server in a separate location, so if for any reason, discovery server is not available it first check the local copy, if first instance of inventory service is not available then it will check the 2nd copy   


create a new module named as discovery-server, add a dependency of Netflix Eureka(EurekaServer), this dependency has the diffenert groupid as org.springframework.cloud, so we need to add the dependency management tag (BOM dependency) copy that and paste it in root pom.xml
and need to provide the version of cloud, so add a tag in properties tag <spring-cloud.version>2021.0.2</spring-cloud.version>
create the pkg com.programming.techie.discoveryserver and create a class DiscoverServerApp and add @SpringBootAppliction, create the main method as we have create a module using maven project so that's why we need to add these things
add @EnableEurekaServer at top of the class 
create the application.properties file 
eureka.instance.hostname = localhost
eureka.client.register-with-eureka = false // don't register itself 
eureka.client.fetch-registry = false //don't fetch the registry of server in eureka server, we will make this field true in eureka client because we want client to fetch registry and store a copy of it.
server.port = 8761 //default port of eureka server 
run the project 

now need to define the eureka clients, add the eureka client dependency in every service we have created and add the annotation @EnableEurekaClient and now need to implement some configuration
eureka.client.serviceurl.defaultZone = http://localhost:8761/eureka       //property is set so that our client can find the eureka server
we can check the services on http://localhost:8761
spring.application.name = inventory-service  //change for others 	



====================================== Mulitple Instances of Inventory Service ==========================
Now as we saw in the theoretical examples we are going to create multiple instances of inventory service
First make the server.port = 0 // Free port (spring boot will pick a random free port and run the inventory service on that port)
select the inventory service from dropdown in IntelliJ -> edit configurations -> Allow parallel run check box, it will allow us to run the multiple instances of application.
now we can replace the hardcoded port num when using WebClient, now we can use the name of the service (inventory-service)
it will the give the error "failed to resolve 'inventory-service' after 9 queries", this is because we have multiple instances now, so client is confused which one to call, it should check the service one by one and try to call one by one, so to enable this functionality we need to configure the client side load balancing in out eureka clients.
so for that we need to add an annotation when constructing the web client bean
replace WebClient with WebClient.Builder, name of method as 'webClientBuilder' and return WebClient.Builder and add the @LoadBalanced annotation at top of the method -> this will enable the client side load balancing capabilities to the web client builder, now even though our order service finds multiple services of the inventory service it will call the services one by one
change the reference in order service class as WebClient.Builder webClientBuilder


============= Destructive Test ========
we will break some thing and will see how our system will behave like we are going to down the eureka server.
stop the discovery server
still we hit the url of order service and our order is placed successfully because we have the local copy of discovery.
now we are going to stop the both instance of inventory service and rerun the inventory serice, it will give the error as it is not able to register itself on eureka servcer because server is down (Transport Exception)
now let's see how our order service will behave, its giving the Internal Server Error, its also give the same exception 'Transport Exception' because order service will first check the local copies of inventory service which we had shut down then order service will try to contact to discovery server to fetch the registry information but its not running that's why its giving error.
now let's start the discovery server, first there is no serive register because clients take almost 30 seconds to register itself to discovery server so wait for them to register.
now hit the order api now it will successfully place the order 





================== API Gateway ================
currect model is when the user is calling the product service, it will hit the url with some random port to access the service, same for other services
This is fine for local development environment but it will not work for production environment as microservices has diffent instances and can run on different ports also, so for this reason we cannot rely on hardcoding these values to call the service
Solution is API Gateway which is responsible to route the request from users to the corresponding service  

API Gateway is like the entry point to our system, if user wants to call the Product Service, the user just call the normal Api gateway with normal url like online-shop.com, to be able to call the product service online-shop.com/api/product then in our API Gatewary we can configure the rules such that if we receive a request with url parameter as /api/product then route it to product service, if its /api/order then route this request to OrderService 

API Gateway can :
Routing based on Request Headers
Authentication  -> API Gateway can communicate with Authorization Server or Authentication Mechanism we want to implement 
Security -> we will make the API Gateway talk to our keyCloak Authorization server and will implement the security aspects
Load Balancing -> If we have multiple instances of Product Service, if user makes a call it will make sure which instance to call to retrieve the response from service and return 
SSL Termination (explore) -> usually when we are making a call to any external service to include the https scheme by following the TLS Protocol, if we call the API Gateway from outside with TLS scheme with https protocol, The API Gateway as its already part of microservice network so we don't need https within our own services which are being routed through API Gateway, we can just perform with simple http so that's why API Gateway terminate the SSL connection.
We have multiple API Gateways - APG, CONG - but we will use the Spring Clouds own implementation Spring Cloud Gateway 

Its not built on top of Spring MVC, its built on top of Spring WebFlux.

Predicates and Filters (explore)

add dependency of Gateway 
create a new maven module, add dependency in it
create package and create a class ApiGatewayApplication 
add the eureka client dependency, configure the application.properties 
eureka.client.serviceUrl.defaultzone = http://localhost:8761/eureka
application.name = api-gateway
@EnableEurekaClient

class responsible for defining the routes and identify the routes for incoming request
logging.level.org.springframework.cloud.gateway.route.RouteDefinitionLocator = INFO
logging.level.root = INFO
logging.level.org.springframework.cloud.gateway = TRACE
  
by adding these info we will have some more logs and we can understand what is going on behind the scene when the request is made on API Gateway 


*Define the Route for Product Service 
spring.cloud.gateway.routes[0].id = product-service
spring.cloud.gateway.routes[0].uri = lb://product-service       -- lb will enable the load balancing abilities
spring.cloud.gateway.routes[0].predicates[0] = Path=/api/product      ----- when we get the api call with this path then the API Gateway will route this particular request to the Product Service 

*Define routes for OrderService     
spring.cloud.gateway.routes[1].id = order-service
spring.cloud.gateway.routes[1].uri = lb://order-service
spring.cloud.gateway.routes[1].predicates[0] = Path=/api/order

Run the api gateway service on 8080 port
hit the GET api on postman http://localhost:8080/api/product  -> this request will go to API Gateway service then it will route the request to corresponding service    


*Dicover Server Route
spring.cloud.gateway.routes[2].id = discovery-server
spring.cloud.gateway.routes[2].uri = lb://localhost:8761
spring.cloud.gateway.routes[2].predicates[0] = Path=/eureka/web

this will give an error NOT FOUND because the API Gateway will call the localhot:8761/eureka/web but we need to route to localhost:8761 -- so here comes the concept of filters
we can modify the url using filters, we will remove the url parameter and change the path 

spring.cloud.gateway.routes[2].filters = SetPath=/

it will again give the error because of 'lb' in uri of route of eureka server as we have only one instance of this server

update 'lb' to 'http' 

now the request is successful but it is giving only html part of the eureka page because we have not configure the route for static files that's why its not loading the css and javascript

so for that we need to configure the route for static resources

spring.cloud.gateway.routes[3].id = discovery-server-static
spring.cloud.gateway.routes[3].uri = lb://localhost:8761
spring.cloud.gateway.routes[3].predicates[0] = Path=/eureka/**

now its fine


=========================== Authorization using KeyCloak Authorization server ===========
Through KeyCloak we can outsource our Authorization and Authentication	


































<<<<<<< HEAD
=======







  














>>>>>>> e0811427ae2ffed7e189f51db30b6a7e49fc5120




<<<<<<< HEAD
============= KeyCLoak ============
run the docker keycloak image as we don't want to download it manually.
docker run -p 8181:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:18.0.0 start-dev

open the http://localhost:8181
Sign in with credential that we have configured while running the docker image 
create a realm as spring-boot-micorservices-realm
Realm -> concept of grouping the users or clients in a single entity

create a client as "spring-boot-client" 
select confidential client as Access Type

disable the standard flow enabled 
disable the Direct access Grants Enabled 
select the service accounts Enabled 

these are the client credentials Grant 
click on save -> this will create the client secret 
go to credentials tab and see the secret 
we can use this client secret for authentication 
after configuring the keycloak we have to provide the url of keycloak in our API Gateway 

for that go to Realm Settings -> click on OpenID Endpoint Configuration -> copy the issuer url -> we need to provide this url inside our api gateway because we have to configure our API Gateway to talk to the KeyCLoak server 

open the pom.xml of API Gateway project -> add dependency oauth2-resource-server  -> remove the version tab if you configure in parent
also add the spring security dependency 


in application.properties file of API Gateway 
spring.security.oauth2.resource.jwt.issuer-uri=http://localhost:8181/realms/spring-boot-microservices-realm

Spring boot will read the dicovery document, and it will fetch all the endpoints to authorize  when starting the application 

we have the token_endpoint api in it where spring boot will make a call to this api and verify the token is valid or not
so instead of configuring all the values in our application.properties, we can just configure the issuer uri and spring boot will do the work behind the scenes by reading the openid-configuration document and will receive all the configurations it needs 

create a SecurityConfig class in config package, add the @Configuration annotation, add the @EnableWebFluxSecurity because spring cloud gateway project is based on SpringCloudWebFlux project not on spring MVC.

create a bean SecurityWebFilterChain with method parameter of ServerHttpSecurity -> inside this method, we have to configure the  webfluxsecurity details
first disable csrf
serverHttpSecurity.csrf.disable().authorizeExchange(exchange -> exchange.pathMatchers("/eureka/**").permitAll().anyExchange().authenticated() ) -> for only eureka static resources we want to permit all the calls but for others make sure those are authenticated
as we are only communicating through rest api through postman client

also we need to enable the resource server capability in our api gateway .oauth2ResourceServer(ServerHttpSecurity.OAuth2ResourceServerSpec::jwt);serverHttpSecurity.build()
All our routes will be protected along with KeyCloak (??)

Protection (CSRF, CORS, session management, etc.) (Explore)


csrf(cross site request forgery)

stateful apps (session-based). 




============= Stateful And Stateless Authentication ==============
In stateful authentication, the server keeps track of user's session.
	When you log in:

		Server creates a session (with a unique ID).	

		This session info (user details, roles, etc.) is stored in server memory or database.

		A session ID (like JSESSIONID) is sent to the browser as a cookie.
	
	On each request:

		The browser automatically sends the session cookie.

		Server looks up the session â†’ finds the logged-in user.
		


In stateless Authentication, the server does not store any user session.
Instead, all information needed to authenticate the user is sent with every request(usually via JWT).
	When you log in:
		Server generates a JWT token (conatins user info, roles, expiry).
		Token is given to the client (browser/mobile app).
	on each request: 
		Client sends JWT in the Authorization: Bearer <token> header.
		Server validates the token (signature and expiry)
		No session storage is needed - every request is self contained 

=========================

* Restart the Api Gateway and hit the api on postman 
Get -> http://localhost:8080/api/product
when you hit, this will give you the 401 error "unauthroize request" because we didn't provide any authentication details, so in this case we have to provide bearer token (jwt token) but for that we have to create the token, we have to request the token from keycloak by providing our credentials then we will provide those credential to api gateway through the authorization header, the type of bearer scheme
go to authorization tab -> select Auth2.0 -> in configuration options  provide the Token Name, Grant Type as Client Credentials(correct grant type when we have machine to machine communication),
in Access token url  http://localhost:8181/realms/spring-boot... (discovery server token-endpoint url)
Client ID -> provide that we have created "spring-cloud-client"
Client secret -> get from keycloak realm (in Clients -> spring-cloud-client -> Credentials -> Secret)
leave scope as it is 
 
Click on get new access token -> once the authentication is completed -> postman will request keycloak for the access token -> click on use token -> now this token will be use whenever we are making a call to our microservices 
now hit the api http://localhost:8080/api/product , we will now get the response 



======== Contact the eureka server through the browser ============
add the dependency in discovery-server  spring security, create the security config class, add the @Configuration @EnableWebSecurity
this class extends WebSecurityConfigurerAdapter 
Override the configure method and provide the parameter of method AuthenticationManagerBuilder

@Value("${eureka.username}")
private String eurekaUsername;

@Value("${eureka.passwords}")
private String eurekaPass;

in method body -> authenticationManagerBuilder.inMemoryAuthentication().passwordEncoder(NoOpPasswordEncoder.getInstance).withUser(eurekaUsername).password(eurekaPass).authorities("USER");
now we have basic auth created in our security config, let's add some additional configuration by overriding another configure method 
with parameter HttpSecurity 
in method body -> httpSecurity.csrf().diable().authorizeRequests().anyRequest().authenticated().and().httpBasic();


dont hardcore the withUser and password value, instead create the 2 properties in application.properties file 
eureka.username = ${EUREKA_USERNAME:eureka}  ->even generalizing, we will access it through environment variables 
eureka.password = ${EUREKA_PASSWORD:password}

we can poss these variables while running the discovery server 
If there are no ENV variables, then the default credentials will be picked up  

other services will give error because they are not authenticate to use the eureka server so we need to provide the credentials
change the serviceUrl.defaultZone = http://username:password@localhost:8761/eureka  
add the same property to all other services  


=========================== Circuit Breaker and replace the WebClient with FeignClient =================

main used when we have a resillient communication between services, problem with Synchoronous communication is that what if one service is down or for some reason the inventory service may delay in response like if db has some issue db or performance issue

when we face these kinds of issue, we need to make resillient it.

Three states of Circuit Breaker
1) CLOSED  2) OPEN 3) HALF_OPEN

when the communication is going smoothly and everything is fine then the circuit breaker is in CLOSED state 
when the communication is disrupted or inventory service is down for some reason then the circuit breaker is in OPEN state. during this period it will through the Exception or use some fallback logic if the Inventory is not UP.
when the state is OPEN after certain duration the circuit breaker will change the status to HALF_OPEN that means it will slowly start taking the request to the inventory service and it will check whether the request are going through or not, if not going it will again change to OPEN state

we will use the Resillience4j used as Circuit Breaks and used for Fault Tolerance. inspired by Netflix Hystrix but designed for Java 8 and functional programming.

add the dependency of spring CloudCircuitBreakerResilience4j and SpringBootStarterActuator in order service 

add Actuator properties 
management.health.circuitbreakers.enabled=true  // enable the circuitbreakers endpoints in Actuator
management.endpoints.web.exposure.include=* // Actuator will expose all kinds of endpoints, by default it will only expose the health points
management.endpoint.health.show-details=always //by default its never


	==== Resillience4j properties ===
resilience4j.circuitbreaker.instances.inventory.registerHealthIndicator=true // enable so that we can see the Health Indicator so that we can different states of circuit breaker
-----------------------------------------------.event-consumer-buffer-size=10 // how much buffer size it is going to take for the events 
-----------------------------------------------.slidingWindowType=COUNT_BASED // CircuitBreaker will not open immediately from CLOSE to OPEN, it must wait for some hits then it will change the state, so we have set this property as COUNT_BASED
-----------------------------------------------.slidingWindowSize=5 // After 5 failed request circuit breaker will change the state from CLOSED to OPEN
-----------------------------------------------.failureRateThreshold=50 // when the 50% are calls are fail
-----------------------------------------------.waitDurationInOpenState=5s // this property ensure the circuit breaker wait for 5 sec in OPEN state before going to OPEN state 
-----------------------------------------------.permittedNumberOfCallsInHalfOpenState=3 // how many calls are allowed in HalfOpenState, like it will communicate with inventory api if it succeed after 3 consecutive calls the state will be change from HALF_OPEN to CLOSED, if 3 consecutive api's are failed then it will change the state from HALF_OPEN to OPEN state 
-----------------------------------------------.automaticTransitionFromOpenToHalfOpenEnabled=true // automatic transition from OPEN to HALF OPEN 



	======== configure the code ==========

add the annotation @CircuitBreaker(name ="inventory", fallbackMethod = "fallbackMethod") at the top level of placeOrder method in OrderController

create the method with return type as String
public String fallbackMethod(OrderRequest orderRequest, RuntimeException runtimeException){
	return "OOPS! something went wrong, please order after some time!";
}


stop the inventory service, check the status of actuator, http://localhost:8081/health, you will see the circuit breaker's state is closed
now after stopping the inventory service, hit the api you will get the fallbackMethod response, now when you will check the state of circuit breaker is still in closed because we have configured that after 5 consecutive failed calls, it will go to OPEN state
after 5 failed calls, you will see the state is changed to OPEN but after 5 sec it will go to HALF_OPEN state

start the inventory service and wait for 30 sec because the service will take that much time to fetch the registry, after the successful hit the state of circuit breaker is still in HALF_OPEN till 3 consecutive success calls


============= now some advance used cases, like in time delay or slow network connection ======
like if order service is communicating with inventory service and for some reason the inventory service is not responding due to some connectivity issue or slow due to db performance issue, so we can introduce a timeout that order service will wait for inventory service for some particular time duration

we need to add some property 
	===== Resillience4j time out property =======
resilience4j.timelimiter.instances.inventory.timeout-duration=3s // so order service will wait for inventory service's response for 3s if not received then it will call the fallback method.

so to implement add the annotation called as @TimeLimiter(name = "inventory") on placeOrder method in OrderController  // same name or key in application.properties and in circuit breaker too

we also need to method signature CompletableFuture<String> in placeOrder and fallbackMethod  
CompletableFuture.supplyAsync(()-> "OOPS! Something went wrong, please place the order after some time!"))

when time limit is reaches it circuit breaker will call the fallback method and through the timeout exception

add the slf4j for logging in InventoryService class @slf4j  and in isInStock method -> 
log.info("Wait Started");
Thread.sleep(10000); //execution will stop for 10 seconds 
log.info("Wait Ended");

so in this way we can simulate the slow behaviour inside the inventory service 
restart the services and access the token to make a call. we get the exception 500 InternalServerError because we have added the 10 sec thread delay in inventory service and circuit breaker has configuration if response is not getting until 3 sec it will call the fallback method 

 	==== Concept of Retry ===
when there is slow or no response coming from target service we would ideally like to retry the call like in this we would like to retry the call to inventory service to isInStock endpoint

Implement the Retry logic
just below the properties of timeout

//Resilience4j Retry Properties
resilience4j.retry.instances.inventory.max-attempts=3 // we have to attempt the retry for 3 times 
--------------------------------------.wait-duration=5s // wait for 5 sec before retrying the call if we don't receive the response in 5 sec we will do a retry for 3 times, if still didn't get the response that will give access to circuit breaker to handle the flow 
add the @Retry(name= "inventory") // similar to circuit and timelimiter

rerun the service

service is retrying after some time to inventory service and waiting for response, not going to fallback method, it will call the inventory service 3 times with duration of 5 sec and still don't get the response then this will give the access to circuit breaker 
check the retry events url in actuator 


============= 

we have the Resillient system but how exactly can be tracked on the issues, but for sure we will use logs, 
but actually in Production grid applications where we receive thousands or logs and then its becomes messy to understand the things or logs so for that we have a design pattern called as Distributed Tracing, which helps us to  track the request from start to finish so that if request is failed at any point of time we can understand why its failed and where 

Let's understand the Distributed Tracing 
User will place an order in our system, so this request will first reach the API Gateway, api gateway will proxy this request or send this request to order service, so order service will make a call to inventory service. so this is our request flow.
so to be able to track the request all the way from API Gateway to Inventory Service we need some kind of mechanism to trace the request
so for this we have traceId in distributing tracing -> unique identifier for request 
along with traceId we also have the SpanId -> spanId is the number of trips the request it going to take inside our system, like we have one trip to API Gateway, order trip to Order service and then lastly to inventory service, for each trip we have unique identifier as span1, span2, span3 and so on.
spanId is unique identifier for each request inside the individual service, but the traceId is a unique identifier for whole request. using these kind of mechanism we can trace the whole reqeust's lifecycle in our services, we can also understand a service is responding slowly or it has some performance issues

	== Implement ==
We will use the spring cloud project called spring cloud sleuth	-> its a distributed tracing framework which helps us to generate the trace and span id whenever we receive a reqeust to our system.
we also need some UI to visulize this information so for that we have Zipkin

add the dependency in API Gateway project of sleuth and sleuth zipkin, paste these dependencies in all projects 

next step is to configure the properties in application.properties but before that download the zipkin
go to zipkin website and download the docker image 

in api gateway properties file
spring.zipkin.base-url=http://localhost:9411
spring.sleuth.sampler.probability=1.0  // we want to send hundred percent of our reqeust that we are receiving to to zipkin

copy and paste these properties in all services

get new access token, hit the apo to product service and in console of product service we will see a unique identifiers, the request contains the span id and trace id

now hit the api of place order, we have configure the timeout logic using circuit breaker so this api will give timeout exception 	


now go to zipkin url and check all the status of api workflow (explore little bit from video)
hit http://localhost:9411/zipkin/
click on run query
it will give you the latest api calls history and flow (all the information)

in first api that we hit to place the order have 3 spans first in API Gateway, 2nd in OrderService and 3rd in InventoryService	


we can confuse in place Order api that we don't have the inventory service in Tracing at zipkin and we have another request that shows the api call to inventory service, this is because we have a different thread for these calls like when we call the order place the API Gateway and Order service is one request and then order service is having the second request to inventory service due to circuit breaker(I think)

Slueth provide us the mechanism to create our own span id 
private final Tracer tracer;
just before making the call to the inventory service 
type 
tracer.nextSpan().name("InventoryServiceLookup");
tracer.withSpan(inventoryServiceLoopup.start());

check next code through video 





============================ Event Driven Architecture using Kafka ====================

what is Event Driver Architecture?

so in Order Service we are communication with inventory service in synchronous manner, like we are making an http call to order service to place the order and order service is again calling the inventory service using the rest api or http call or rest call, so this is synchronous communication

we can have another kind of communication called as Asynchronous communication where the order service will make the request to inventory service and it will not wait for inventory service
this kind of communication can be enable through event driven communication, it is nothing  but performing Asynchronous communication in the form of events.
so whenever we receive an order to order service and order place successfully, our order service will raise and event, in this scenario it can be like OrderPlacedEvent and we can place this OrderPlacedEvent object as a message inside the kafka broker and our notification service which is the consumer, our order service is the producer of the message and notification service will be the consumer for that message, it will consume the message and process this message accordingly like sending message in the form of message or email but we are not going to implement this functionality in the notification service so we are going to setup this event driven architecture using kafka.

for installation we will use the docker compose  
open the Apache kafka Quick Start guide from Confluent Developer website -> go to docker, copy the docker compose file 

create a file docker-compose.yml and paste the docker kafka code from that website
we have 2 service in docker file, first one is zookeeper is used to orchestrate the kafka clusters, zooker_tick_time is the interval that the zoo keeper will send the heart beat messages something like eureka is doing. 
next is kafka broker 
open the terminal and type docker compose up -d, you can check the status of docker container by running docker ps command 

so library is Spring for Apache Kafka provides development of kafka-based messaging solutions, it also support the Message-driver POJOs with @KafkaListener annotation and a listener container.
kafkaTemplate class provide the functionality to allow the producers (in our case order service) can send messages to our consumers(notification service) and from consumer side we can define the consumer as a Listener by adding this kafkaListener annotation  

now need a dependency -> go to learn -> reference doc -> introduction -> quick tour -> copy the maven dependency and paste in pom.xml of the order service, remove the version.

now configure our order service to listen to the kafka broker which is running on our machine
application.properties :


spring.kafka.bootstrap-server=localhost:9092     //list of servers where we can find kafka installation, this property will configure the kafka in our project.

inside order service we are going to make a call to kafka cluster whenever the place will be placed so for that inject the kafkaTemplate 

private final  KafkaTemplate<String,OrderPlacedEvent> kafkaTemplate;

and just after the orderRepository.save method  
kafkaTemplate.send("notificationTopic", new OrderNumber(order.getOrderNumber))   // instead of sending the order.getOrderNumber() we can create a class to send the JSON object like a DTO. create a new package called as event inside that pkg create a class as OrderPlacedEvent -> has only orderNumber field for now

Now kafkaTemplate will send this OrderPlacedEvent object as a message to the notificationTopic

we can also define the notificationTopic as a default topic for our order service, so if we want to send multiple messages to kafka so that spring boot will understand that it has to always send the messages to notificationTopic 

add property
spring.kafka.template.default-topic=notificationTopic

we also have to provide information to our spring boot project that how to serialize these key value pair when sending to the kafka broker, so we can define some serializers through spring kafka project 
add the property 
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer

likewise we also have to define the serializer for value that is the OrderPlacedEvent so by default its okay if you want to send the OrderPlacedEvent as a JSON object so to be able to convert this java object to JSON we have add property 
spring.kafka.producer.value-serializer=org.springgframework.kafka.support.serializer.JsonSerializer


========= kafka consumer ====
now go ahead and create a consumer which is notification service 
create a notification-service project from spring initializer -> add dependency -> Lombok, spring web, apache kafka , eureka service client, zipkin, sleuth, 
copying the dependencies because we will create the maven module and add things manually 

click on parent project -> new -> module -> maven -> next -> provide name as notification-service -> and make sure you have the parent as Microservices-new
create a package com.programming.techie and create a NotificationServiceAppication class and paste the code or write (main class)

now we need to define the kafka listener inside main class 

@KafkaListener(topics = "notificationTopic")
public void handle notification(OrderPlacedEvent  orderPlaceEvent){ //this method will take the OrderPlacedEvent as method parameter, create class OrderPlacedEvent
// send out the email notification 
log.info("Received notification for Order - {}",orderPlacedEvent.getOrderNumber());
}


so whenever a message is placed or an orderPlaceEvent is raised we are going to place the message from our order service into the kafkaTopic (notificationTopic) and the notification service is going to listen to this notificationTopic and its going to print out the message 

need to configure the notification service in application.properties
copy the eureka, zipkin, sleuth, kafka (just change serializer to deserializer from consumer side e.g notification-service) configuration
server.port=0 
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serialization.JsonDeserializer


but from order service the fully classified class name of OrderPlacedEvent is com.programmingtechie.orderservice.event.OrderPlacedEvent
 and in consumer side its com.programming.techie.OrderPlacedEvent so spring boot will be confuse when deserialize this object, so we have to provide the mapping type on producer side and on consumer side too


IN order service
spring.kafka.producer.properties.spring.json.type.mapping=event:com.programmingtechie.orderservice.event.OrderPlacedEvent   //now spring boot will understand what is the JSON type mapping of the OrderPlaceEvent


In notification service 
spring.kafka.consumer.properties.spring.json.type.mapping=event:com.programmingtechie.OrderPlacedEvent 


starting the application will give an exception IllegalStateException as we have not define the groupid for the consumer 
add spring.kafka.consumer.group-id=notificationId








======================= Dockerize the Project =================

First step is to dockerize all of our services and then run them together using tool called as docker compose 

Install the docker desktop after installing we also need to register on Docker Hub -> this is where we push our docker images after building them on our local machine, Register for an account
Sign in to docker desktop

so let's start dockerizing the services of our projects
starts with ApiGateway -> Create Dockerfile and Dockerfile.layered in root folder of api-gateway project 

DockerFile ->

FROM openjdk:17  // OpenJDK:17 -> base image 

COPY target/*.jar app.jar  //copying the jar file from target folder in to container and we are calling this jar file as app.jar

ENTRYPOINT ["java","-jar","/app.jar"]  //Entry point command, so whenever a container is running up or starting up it will run the command java -jar  and it will point to this app.jar file 


open the terminal at api-gateway 

to build this docker file and we have to provide the location where the docker file exists 
docker build -t apigateway-dockerfile . // -t is tag for image, . -> means currect folder 

click enter and it will try to download the OpenJDK:17 image from docker hub 

let's see the image
docker images 

docker image is too large, we can do some best practices like we can use openjre image instead of OpenJDK 

and when we will make change in our api-gateway project like we update one dependency then the docker file will build the whole image one more time, it does not have any contextual understanding or what is the only part we should build, for this reason docker has a concept called Multi Stage Build.

Let's see how to improve our docker file using Multi Stage Build Mechanism 
so for that we have created another file called as dockerfile.layered 

FROM eclipse-temurin:17.0.4.1_1-jre as builder       // we are using this eclipse-temurin jre instead of using the openjdk and giving it alias as builder
WORKDIR extracted         // creating a work directory as extracted inside the container
ADD target/*.jar app.jar          // we are adding this jar which inside the target folder into the container and calling it as app.jar
RUN java -Djarmode=layertools -jar app.jar extract      // as part of java 9 or 10 java supports extracting different layers from the jar files, our java application is divided in to different layers  (It doesnâ€™t run your app yet â€” instead, it â€œextractsâ€ your fat jar into layers) (extract command at the last is used from extracting all the layers from jar file and store it inside the extracted folder)

* This is the first stage of docker build we want to do 
* below the second stage 

--This stage only contains whatâ€™s needed to run your app (no build tools, no Maven, nothing extra)--
FROM eclipse-temurin:17.0.4.1_1-jre  // says use this java official Java17 image 
WORKDIR application
COPY --from=builder extracted/dependencies/ ./
COPY --from=builder extracted/spring-boot-loader/ ./                         (copy these four necessary layers or folders from the extracted folder into our application folder)
COPY --from=builder extracted/snapshot-dependencies/ ./
COPY --from=builder extracted/application/ ./
EXPOSE 8080     // exposing 8080, tells docker that our application will listen on port 8080 (Doesnâ€™t actually open it, just documents it for Docker)
ENTRYPOINT ["java","org.springframework.boot.loader.JarLaucher"]           //Define the command to run when the container starts, here it runs the Spring boot launcher that knows how to stitch back the layers into running app.



now if we change just version inside the dependencies, docker will understand that okay only file which is changed is inside the dependencies folder so let's build only this particular folder or just copy dependencies folder in to our application directory and rest of all will be the same, so in this way we can save the build times 

Dockerfile tells Docker how to build the docker image(like a packaged environment) that can run your Java (Spring boot application)



Okay so go ahead and build this Dockerfile.layered
open the terminal \api-gateway> docker build -t apigateway-layered -f Dockerfile.layered . (if we dont mention the layered file, it will build the docker file)  
it will build the docker image in multiple stages 

check the image
docker images 
last time when we build image using docker file has more MBs than Dockerfile.layered file, so the size becomes less and we can also divided the folders in mutliple stages


Next step is how to build the docker images from source code without using any docker files, how to build the docker images automatically

we can make use of library called Jib : Jib handles all steps of packaging your application into a container image. You don't need to know best practices for creating Dokerfiles or have Docker installed.
used to build containers from our java applications without using docker file or installing docker 





open the pom.xml of root or parent project 

add a plugin of jib (check video or explore from jib documentation)
after adding this plugin, we have to click on jib build button, you can from maven button at right in IntelliJ IDE >plugins>jib>jib:build
jib will create dockerfile and create the image and push it on docker hub

mvn clean compile jib:build

this will give error 401 Unauthorized exception because we are pushing this image to our docker hub but we didn't provide our docker hub credentials
open the setting.xml file and add <servers> <server> <id> registry.hub.docker.com</id> <username> your docker hub username</username> <password> your docker hub password </password>


now again run the command mvn clean compile jib:build.


verify the images docker images also verify these images are pushed on to the docker hub or not 
sign in to docker hub and you will see the services now we can make use of this images and download all these images and run those containers using docker compose 


--- Docker compose file ---

so now we have all our images in to the docker hub, next step is to create the docker compose file

refer the docker-compose.yml file under the root folder of microservices project

In our Architecture of project we are using the MySQL, MongoDB, AuthServer (keycloak also uses MySQL), zookeeper and kafka, zipkin so first of all we need to setup these external services in our docker-compose file  
we are using the postgreSQL instead of mysql



version: '3'

services: 
	(explore this from video) 


====Explore docker-compose tutorial====
After docker-compose file
Add the data.sql files under src/main/resource folder for both inventory and order services
CREATE DATABASE IF NOT EXISTS 'order-service'
CREATE DATABASE IF NOT EXISTS 'inventory-service'

run the command
docker compose up -d   //this will pull all the images which are required for all the services defined inside the docker-compose file and it will run containers for those images in a Daemon mode 
once they are up and running we can go to our postman and start making calls to our microservices project 

check the logs -> docker logs -f broker  // logs of kafka broker  

check the logs -> docker logs -f order-service  // logs of order service 


go to postman and get the access token from the keycloak 

but we need Client secret for defined Client id because now the application is running on docker and whenever we restart the docker, docker will create new client Secret
so for that first we have to open our keycloak, login with configured credentials in docker-compose file, regenerate the secret and copy and replace the client secret in postman 

Authorization tab in postman -> paste the new generated client secret -> click on Get New Access Token -> click on user this token -> hit the api

got 401 unauthorized exception, we are checking the logs, first we are checking the API-GATEWAY because it is the starting point of application , it is giving error Authentication Failes: The iss claim is not valid
go to jwt.io and paste the token you will get the structure data and check what is iss 
so iss has the url of localhost so application is trying to hit the api on localhost , we need to change it as keycloak 
so in Authorization tab in postman  we have the Access Token URL as localhost, replace it with keycloak then it will point to keycloak which is running in our machine 

Still the Authentication Failed because we are trying to contact the docker container directly with the hostname, so to be able to contact this docker container from our host we need to do some small changes in our system, open the file name as hosts and we have to inform our windows host to redirect all the traffics from keycloak to a localhost (file directory -> 	windows/system32/drivers/etc/host) -> need to change it in administrator mode  
add -> 127.0.0.1 keycloak  

now our postman will be able to contact the keycloak docker container 
now click on Get New Access Token  -> now the Authentication will be completed -> by adding this particular line we are able to communicate from our localhost to the docker host to the hostname otherwise there will be some DNS problems 
keep in mind we need this when we need to communicate the docker container from our local and we don't need to do this when we are doing it in production

now we are getting the response 

 


















































